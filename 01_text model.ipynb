{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\ten1.9\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudart_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 지정된 모듈을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-a7168d4006d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\ten1.9\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\ten1.9\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\ten1.9\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\ten1.9\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m               \u001b[1;34m\"environment variable. Download and install CUDA %s from \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m               \u001b[1;34m\"this URL: https://developer.nvidia.com/cuda-toolkit\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m               % (build_info.cudart_dll_name, build_info.cuda_version_number))\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m       if hasattr(build_info, \"cudnn_dll_name\") and hasattr(\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk_data.corpora import wordnet\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEMOCAP Dataset - Emotion extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = 'C:\\python\\\\ten1.9\\\\google_stt\\\\dataset\\\\data\\\\Session1\\\\dialog\\\\EmoEvaluation\\\\Ses01F_impro01.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[START_TIME - END_TIME] TURN_NAME EMOTION [V, A, D]\\n',\n",
       " '[6.2901 - 8.2357]\\tSes01F_impro01_F000\\tneu\\t[2.5000, 2.5000, 2.5000]\\n',\n",
       " '[10.0100 - 11.3925]\\tSes01F_impro01_F001\\tneu\\t[2.5000, 2.5000, 2.5000]\\n',\n",
       " '[14.8872 - 18.0175]\\tSes01F_impro01_F002\\tneu\\t[2.5000, 2.5000, 2.5000]\\n',\n",
       " '[19.2900 - 20.7875]\\tSes01F_impro01_F003\\txxx\\t[2.5000, 3.0000, 3.0000]\\n',\n",
       " '[21.3257 - 24.7400]\\tSes01F_impro01_F004\\txxx\\t[2.5000, 3.0000, 2.5000]\\n',\n",
       " '[27.4600 - 31.4900]\\tSes01F_impro01_F005\\tneu\\t[2.5000, 3.5000, 2.0000]\\n',\n",
       " '[38.9650 - 43.5900]\\tSes01F_impro01_F006\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[46.5800 - 52.1900]\\tSes01F_impro01_F007\\tfru\\t[2.5000, 3.5000, 3.5000]\\n',\n",
       " '[56.1600 - 58.8225]\\tSes01F_impro01_F008\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[61.8700 - 65.9700]\\tSes01F_impro01_F009\\tfru\\t[2.0000, 3.5000, 3.0000]\\n',\n",
       " '[66.4200 - 69.3400]\\tSes01F_impro01_F010\\txxx\\t[1.5000, 3.5000, 3.5000]\\n',\n",
       " '[72.4500 - 82.2600]\\tSes01F_impro01_F011\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[85.2700 - 88.0200]\\tSes01F_impro01_F012\\tang\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[93.6700 - 97.0218]\\tSes01F_impro01_F013\\tfru\\t[2.0000, 4.0000, 3.5000]\\n',\n",
       " '[97.8900 - 102.9600]\\tSes01F_impro01_F014\\tneu\\t[2.5000, 3.5000, 3.5000]\\n',\n",
       " '[103.9700 - 106.7100]\\tSes01F_impro01_F015\\tfru\\t[2.0000, 3.5000, 3.0000]\\n',\n",
       " '[7.5712 - 10.4750]\\tSes01F_impro01_M000\\tfru\\t[2.5000, 2.0000, 2.5000]\\n',\n",
       " '[10.9266 - 14.6649]\\tSes01F_impro01_M001\\tfru\\t[2.5000, 2.0000, 2.5000]\\n',\n",
       " '[16.8352 - 19.7175]\\tSes01F_impro01_M002\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[23.4700 - 28.0300]\\tSes01F_impro01_M003\\tfru\\t[2.5000, 3.5000, 3.5000]\\n',\n",
       " '[28.3950 - 31.2117]\\tSes01F_impro01_M004\\tfru\\t[2.0000, 4.0000, 3.5000]\\n',\n",
       " '[31.2660 - 39.3875]\\tSes01F_impro01_M005\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[41.2300 - 46.9800]\\tSes01F_impro01_M006\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[51.4000 - 57.6400]\\tSes01F_impro01_M007\\tfru\\t[2.5000, 3.5000, 3.0000]\\n',\n",
       " '[58.1800 - 62.5900]\\tSes01F_impro01_M008\\tfru\\t[2.0000, 3.5000, 3.5000]\\n',\n",
       " '[65.5100 - 73.0000]\\tSes01F_impro01_M009\\tfru\\t[2.5000, 3.5000, 4.0000]\\n',\n",
       " '[81.5900 - 86.0300]\\tSes01F_impro01_M010\\tfru\\t[2.0000, 4.5000, 4.0000]\\n',\n",
       " '[87.1500 - 94.3900]\\tSes01F_impro01_M011\\tang\\t[2.0000, 4.0000, 4.0000]\\n',\n",
       " '[95.8600 - 98.6800]\\tSes01F_impro01_M012\\txxx\\t[3.0000, 1.5000, 2.0000]\\n',\n",
       " '[101.8400 - 107.8700]\\tSes01F_impro01_M013\\tang\\t[2.0000, 4.5000, 4.5000]\\n']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_regex = re.compile(r'\\[.+\\]\\n', re.IGNORECASE) #특수문자에서 시작하여 \\n까지 검사\n",
    "\n",
    "with open(file_path) as f:\n",
    "    file_content = f.read()\n",
    "    \n",
    "info_lines = re.findall(useful_regex, file_content)\n",
    "\n",
    "info_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[6.2901 - 8.2357]', 'Ses01F_impro01_F000', 'neu', '[2.5000, 2.5000, 2.5000]']\n",
      "['[10.0100 - 11.3925]', 'Ses01F_impro01_F001', 'neu', '[2.5000, 2.5000, 2.5000]']\n",
      "['[14.8872 - 18.0175]', 'Ses01F_impro01_F002', 'neu', '[2.5000, 2.5000, 2.5000]']\n",
      "['[19.2900 - 20.7875]', 'Ses01F_impro01_F003', 'xxx', '[2.5000, 3.0000, 3.0000]']\n",
      "['[21.3257 - 24.7400]', 'Ses01F_impro01_F004', 'xxx', '[2.5000, 3.0000, 2.5000]']\n",
      "['[27.4600 - 31.4900]', 'Ses01F_impro01_F005', 'neu', '[2.5000, 3.5000, 2.0000]']\n",
      "['[38.9650 - 43.5900]', 'Ses01F_impro01_F006', 'fru', '[2.0000, 3.5000, 3.5000]']\n",
      "['[46.5800 - 52.1900]', 'Ses01F_impro01_F007', 'fru', '[2.5000, 3.5000, 3.5000]']\n",
      "['[56.1600 - 58.8225]', 'Ses01F_impro01_F008', 'fru', '[2.0000, 3.5000, 3.5000]']\n"
     ]
    }
   ],
   "source": [
    "for l in info_lines[1:10]:\n",
    "    print(l.strip().split('\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text data and emotion extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Text data and emotion extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_line = re.compile(r'\\[.+\\]\\n', re.IGNORECASE)\n",
    "\n",
    "start_times, end_times,wav_file_names,emotions,vals,acts,doms = [],[],[],[],[],[],[]\n",
    "\n",
    "for sess in [5]:\n",
    "    emo_dataset_dir = 'C:\\python\\\\ten1.9\\\\google_stt\\\\dataset\\\\data\\\\Session{}\\\\dialog\\\\EmoEvaluation\\\\'.format(sess)\n",
    "    evaluation_files = [l for l in os.listdir(emo_dataset_dir) if 'Ses' in l]\n",
    "    for file in evaluation_files:\n",
    "        with open(emo_dataset_dir + file) as f:\n",
    "            content = f.read()\n",
    "        info_lines = re.findall(info_line, content)\n",
    "        for line in info_lines[1:]:  # the first line is a header\n",
    "            start_end_time, wav_file_name, emotion, val_act_dom = line.strip().split('\\t')\n",
    "            start_time, end_time = start_end_time[1:-1].split('-')\n",
    "            val, act, dom = val_act_dom[1:-1].split(',')\n",
    "            val, act, dom = float(val), float(act), float(dom)\n",
    "            start_time, end_time = float(start_time), float(end_time)\n",
    "            start_times.append(start_time)\n",
    "            end_times.append(end_time)\n",
    "            wav_file_names.append(wav_file_name)\n",
    "            emotions.append(emotion)\n",
    "            vals.append(val)\n",
    "            acts.append(act)\n",
    "            doms.append(dom)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>val</th>\n",
       "      <th>act</th>\n",
       "      <th>dom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6132</td>\n",
       "      <td>6.17</td>\n",
       "      <td>Ses05F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.1500</td>\n",
       "      <td>19.49</td>\n",
       "      <td>Ses05F_impro01_F001</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.8500</td>\n",
       "      <td>26.90</td>\n",
       "      <td>Ses05F_impro01_F002</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.9800</td>\n",
       "      <td>34.46</td>\n",
       "      <td>Ses05F_impro01_F003</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.4800</td>\n",
       "      <td>42.18</td>\n",
       "      <td>Ses05F_impro01_F004</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time             wav_file emotion  val  act  dom\n",
       "0      3.6132      6.17  Ses05F_impro01_F000     neu  4.0  2.5  3.0\n",
       "1     14.1500     19.49  Ses05F_impro01_F001     fru  2.5  3.0  3.0\n",
       "2     22.8500     26.90  Ses05F_impro01_F002     fru  2.5  3.0  3.5\n",
       "3     29.9800     34.46  Ses05F_impro01_F003     fru  2.5  3.0  3.0\n",
       "4     39.4800     42.18  Ses05F_impro01_F004     fru  2.5  3.5  3.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion = pd.DataFrame(columns=['start_time', 'end_time', 'wav_file', 'emotion', 'val', 'act', 'dom'])\n",
    "df_emotion['start_time'] = start_times\n",
    "df_emotion['end_time'] = end_times\n",
    "df_emotion['wav_file'] = wav_file_names\n",
    "df_emotion['emotion'] = emotions\n",
    "df_emotion['val'] = vals\n",
    "df_emotion['act'] = acts\n",
    "df_emotion['dom'] = doms\n",
    "\n",
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.to_csv('./dataset/preprocess/emotion_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Text data and emotion extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ses01F_impro01_F000 [006.2901-008.2357]: Excuse me.\\nSes01F_impro01_M000 [007.5712-010.4750]: Do you have your forms?\\nSes01F_impro01_F001 [010.0100-011.3925]: Yeah.\\nSes01F_impro01_M001 [010.9266-014.6649]: Let me see them.\\nSes01F_impro01_F002 [014.8872-018.0175]: Is there a problem?\\nSes01F_impro01_M002 [016.8352-019.7175]: Who told you to get in this line?\\nSes01F_impro01_F003 [019.2900-020.7875]: You did.\\nSes01F_impro01_F004 [021.3257-024.7400]: You were standing at the beginning and you directed me.\\nSes01F_impro01_M003 [023.4700-028.0300]: Okay. But I didn't tell you to get in this line if you are filling out this particular form.\\nSes01F_impro01_F005 [027.4600-031.4900]: Well what's the problem?  Let me change it.\\nSes01F_impro01_M004 [028.3950-031.2117]: This form is a Z.X.four.\\nSes01F_impro01_M005 [031.2660-039.3875]: You can't--  This is not the line for Z.X.four.  If you're going to fill out the Z.X.four, you need to have a different form of ID.\\nSes01F_impro01_F006 [038.9650-043.5900]: What?  I'm getting an ID.  This is why I'm here.  My wallet was stolen.\\nSes01F_impro01_M006 [041.2300-046.9800]: No. I need another set of ID to prove this is actually you.\\nSes01F_impro01_F007 [046.5800-052.1900]: How am I supposed to get an ID without an ID?  How does a person get an ID in the first place?\\nSes01F_impro01_M007 [051.4000-057.6400]: I don't know.  But I need an ID to pass this form along.  I can't just send it along without an ID.\\nSes01F_impro01_F008 [056.1600-058.8225]: I'm here to get an ID.\\nSes01F_impro01_M008 [058.1800-062.5900]: No.  I need another ID, a separate one.\\nSes01F_impro01_F009 [061.8700-065.9700]: Like what?  Like a birth certificate?\\nSes01F_impro01_M009 [065.5100-073.0000]: A birth certificate, a passport...a student ID; didn't you go to school?  Anything?\\nSes01F_impro01_F010 [066.4200-069.3400]: Who the hell has a birth certificate?\\nSes01F_impro01_F011 [072.4500-082.2600]: Yes but my wallet was stolen, I don't have anything.  I don't have any credit cards, I don't have my ID.  Don't you have things on file here?\\nSes01F_impro01_M010 [081.5900-086.0300]: Yeah.  We keep it on file, but we need an ID to access that file.\\nSes01F_impro01_F012 [085.2700-088.0200]: That's out of control.\\nSes01F_impro01_M011 [087.1500-094.3900]: I don't understand why this is so complicated for people when they get here.  It's just a simple form.  I just need an ID.\\nSes01F_impro01_F013 [093.6700-097.0218]: How long have you been working here?\\nSes01F_impro01_M012 [095.8600-098.6800]: Actually too long.\\nSes01F_impro01_F014 [097.8900-102.9600]: Clearly.  You know, do you have like a supervisor or something?\\nSes01F_impro01_M013 [101.8400-107.8700]: Yeah.  Do you want to see my supervisor?  Huh? Yeah.  Do you want to see my supervisor?  Fine.  I'll be right back.\\nSes01F_impro01_F015 [103.9700-106.7100]: That would - I would appreciate that.  Yeah.\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdata_path = 'C:\\python\\\\ten1.9\\\\google_stt\\\\dataset\\\\data\\Session1\\dialog\\\\transcriptions\\\\Ses01F_impro01.txt' #예시 텍스트 데이터 경로\n",
    "\n",
    "useful_regex = re.compile(r'.*[.+\\S]\\n', re.IGNORECASE) # 정규식 생성 (모든문자에 대해서 반복하되 \\n까지 검출)\n",
    "\n",
    "with open(textdata_path) as f:\n",
    "    file_content = f.read()\n",
    "    \n",
    "info_line = re.findall(useful_regex, file_content) #위의 정규식에 적합한 데이터 찾기\n",
    "\n",
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006.2901 008.2357\n",
      "007.5712 010.4750\n",
      "010.0100 011.3925\n",
      "010.9266 014.6649\n",
      "014.8872 018.0175\n",
      "016.8352 019.7175\n",
      "019.2900 020.7875\n",
      "021.3257 024.7400\n",
      "023.4700 028.0300\n",
      "027.4600 031.4900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for l in info_line[0:10]:\n",
    "    start_end_time = l.split(':')[0].strip( )\n",
    "    start_end_time = start_end_time.split(' ')[1].strip( )\n",
    "    a,b = start_end_time[1:-1].split('-')\n",
    "    print(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_line = re.compile(r'.*[.+\\S]', re.IGNORECASE)\n",
    "\n",
    "file_name_line = re.compile(r'\\A\\S*', re.I)\n",
    "start_end_line = re.compile(r'\\[.+]', re.I)\n",
    "\n",
    "wav_file_names, start_times, end_times, texts = [],[],[],[]\n",
    "\n",
    "for sess in [5]:\n",
    "    text_dataset_dir = 'C:\\python\\\\ten1.9\\\\google_stt\\\\dataset\\\\data\\Session{}\\dialog\\\\transcriptions\\\\'.format(sess)#텍스트 데이터셋 디렉토리\n",
    "    text_files = [l for l in os.listdir(text_dataset_dir) if 'Ses' in l]#텍스트 파일 리스트 생성\n",
    "    \n",
    "    for file in text_files:\n",
    "        with open(text_dataset_dir + file) as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        text_lines = re.findall(text_line, content)\n",
    "        \n",
    "        for line in text_lines[0:]: \n",
    "            start_end_time = line.split(':')[0].strip( )\n",
    "            start_end_time = start_end_time.split(' ')[1].strip( )\n",
    "            start_time, end_time= start_end_time[1:-1].split('-')\n",
    "            \n",
    "            wav_file_name = re.search(file_name_line,line)\n",
    "            wav_file_name = wav_file_name.group()\n",
    "            \n",
    "            text = line.split(':')[1].strip( )\n",
    "            \n",
    "            start_times.append(start_time)\n",
    "            end_times.append(end_time)\n",
    "            texts.append(text)\n",
    "            wav_file_names.append(wav_file_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003.6132</td>\n",
       "      <td>006.1700</td>\n",
       "      <td>Ses05F_impro01_F000</td>\n",
       "      <td>Hi, I need an ID.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005.7800</td>\n",
       "      <td>014.7800</td>\n",
       "      <td>Ses05F_impro01_M000</td>\n",
       "      <td>ahh Yeah, this is the wrong line.  I'm sorry. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013.0388</td>\n",
       "      <td>013.8112</td>\n",
       "      <td>Ses05F_impro01_FXX0</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>014.1500</td>\n",
       "      <td>019.4900</td>\n",
       "      <td>Ses05F_impro01_F001</td>\n",
       "      <td>Okay, I'm sorry, but I just stood in this line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017.6400</td>\n",
       "      <td>024.1600</td>\n",
       "      <td>Ses05F_impro01_M001</td>\n",
       "      <td>I mean, there's really nothing I can do for yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_time  end_time             wav_file  \\\n",
       "0   003.6132  006.1700  Ses05F_impro01_F000   \n",
       "1   005.7800  014.7800  Ses05F_impro01_M000   \n",
       "2   013.0388  013.8112  Ses05F_impro01_FXX0   \n",
       "3   014.1500  019.4900  Ses05F_impro01_F001   \n",
       "4   017.6400  024.1600  Ses05F_impro01_M001   \n",
       "\n",
       "                                                text  \n",
       "0                                  Hi, I need an ID.  \n",
       "1  ahh Yeah, this is the wrong line.  I'm sorry. ...  \n",
       "2                                                No.  \n",
       "3  Okay, I'm sorry, but I just stood in this line...  \n",
       "4  I mean, there's really nothing I can do for yo...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.DataFrame(columns=['start_time', 'end_time', 'wav_file', 'text'])\n",
    "df_text['start_time'] = start_times\n",
    "df_text['end_time'] = end_times\n",
    "df_text['wav_file'] = wav_file_names\n",
    "df_text['text'] = texts\n",
    "\n",
    "\n",
    "df_text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.to_csv('./dataset/preprocess/text_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 data fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6132</td>\n",
       "      <td>6.1700</td>\n",
       "      <td>Ses05F_impro01_F000</td>\n",
       "      <td>Hi, I need an ID.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7800</td>\n",
       "      <td>14.7800</td>\n",
       "      <td>Ses05F_impro01_M000</td>\n",
       "      <td>ahh Yeah, this is the wrong line.  I'm sorry. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.0388</td>\n",
       "      <td>13.8112</td>\n",
       "      <td>Ses05F_impro01_FXX0</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.1500</td>\n",
       "      <td>19.4900</td>\n",
       "      <td>Ses05F_impro01_F001</td>\n",
       "      <td>Okay, I'm sorry, but I just stood in this line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.6400</td>\n",
       "      <td>24.1600</td>\n",
       "      <td>Ses05F_impro01_M001</td>\n",
       "      <td>I mean, there's really nothing I can do for yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time             wav_file  \\\n",
       "0      3.6132    6.1700  Ses05F_impro01_F000   \n",
       "1      5.7800   14.7800  Ses05F_impro01_M000   \n",
       "2     13.0388   13.8112  Ses05F_impro01_FXX0   \n",
       "3     14.1500   19.4900  Ses05F_impro01_F001   \n",
       "4     17.6400   24.1600  Ses05F_impro01_M001   \n",
       "\n",
       "                                                text  \n",
       "0                                  Hi, I need an ID.  \n",
       "1  ahh Yeah, this is the wrong line.  I'm sorry. ...  \n",
       "2                                                No.  \n",
       "3  Okay, I'm sorry, but I just stood in this line...  \n",
       "4  I mean, there's really nothing I can do for yo...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = pd.read_csv('./dataset/preprocess/text_data.csv')\n",
    "emotion_data = pd.read_csv('./dataset/preprocess/emotion_data.csv')\n",
    "\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>val</th>\n",
       "      <th>act</th>\n",
       "      <th>dom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6132</td>\n",
       "      <td>6.17</td>\n",
       "      <td>Ses05F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.1500</td>\n",
       "      <td>19.49</td>\n",
       "      <td>Ses05F_impro01_F001</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.8500</td>\n",
       "      <td>26.90</td>\n",
       "      <td>Ses05F_impro01_F002</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.9800</td>\n",
       "      <td>34.46</td>\n",
       "      <td>Ses05F_impro01_F003</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.4800</td>\n",
       "      <td>42.18</td>\n",
       "      <td>Ses05F_impro01_F004</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time             wav_file emotion  val  act  dom\n",
       "0      3.6132      6.17  Ses05F_impro01_F000     neu  4.0  2.5  3.0\n",
       "1     14.1500     19.49  Ses05F_impro01_F001     fru  2.5  3.0  3.0\n",
       "2     22.8500     26.90  Ses05F_impro01_F002     fru  2.5  3.0  3.5\n",
       "3     29.9800     34.46  Ses05F_impro01_F003     fru  2.5  3.0  3.0\n",
       "4     39.4800     42.18  Ses05F_impro01_F004     fru  2.5  3.5  3.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_data = pd.merge(emotion_data,text_data, how=\"inner\" ,on=[\"start_time\",\"end_time\",\"wav_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>val</th>\n",
       "      <th>act</th>\n",
       "      <th>dom</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6132</td>\n",
       "      <td>6.17</td>\n",
       "      <td>Ses05F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hi, I need an ID.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.1500</td>\n",
       "      <td>19.49</td>\n",
       "      <td>Ses05F_impro01_F001</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Okay, I'm sorry, but I just stood in this line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.8500</td>\n",
       "      <td>26.90</td>\n",
       "      <td>Ses05F_impro01_F002</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>No, they told me-I'm sorry, but they told me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.9800</td>\n",
       "      <td>34.46</td>\n",
       "      <td>Ses05F_impro01_F003</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>What, I mean what... what's the difference?  W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.4800</td>\n",
       "      <td>42.18</td>\n",
       "      <td>Ses05F_impro01_F004</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Can you just-can I just get the right-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>236.5700</td>\n",
       "      <td>244.83</td>\n",
       "      <td>Ses05M_script03_2_M041</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You are a vile tempered, wicked living, evil l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>244.8400</td>\n",
       "      <td>246.58</td>\n",
       "      <td>Ses05M_script03_2_M042</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Oh, you're not going like this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>246.5900</td>\n",
       "      <td>248.83</td>\n",
       "      <td>Ses05M_script03_2_M043</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[GARBAGE] No, you're not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>255.8600</td>\n",
       "      <td>260.33</td>\n",
       "      <td>Ses05M_script03_2_M044</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>oh! Marry you again? I wouldn't marry you agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>260.3400</td>\n",
       "      <td>266.35</td>\n",
       "      <td>Ses05M_script03_2_M045</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You're a wicked little vampire.  And I pray to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2158 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      start_time  end_time                wav_file emotion  val  act  dom  \\\n",
       "0         3.6132      6.17     Ses05F_impro01_F000     neu  4.0  2.5  3.0   \n",
       "1        14.1500     19.49     Ses05F_impro01_F001     fru  2.5  3.0  3.0   \n",
       "2        22.8500     26.90     Ses05F_impro01_F002     fru  2.5  3.0  3.5   \n",
       "3        29.9800     34.46     Ses05F_impro01_F003     fru  2.5  3.0  3.0   \n",
       "4        39.4800     42.18     Ses05F_impro01_F004     fru  2.5  3.5  3.0   \n",
       "...          ...       ...                     ...     ...  ...  ...  ...   \n",
       "2153    236.5700    244.83  Ses05M_script03_2_M041     ang  1.0  4.5  5.0   \n",
       "2154    244.8400    246.58  Ses05M_script03_2_M042     ang  1.0  4.5  4.5   \n",
       "2155    246.5900    248.83  Ses05M_script03_2_M043     ang  1.5  4.0  4.5   \n",
       "2156    255.8600    260.33  Ses05M_script03_2_M044     ang  1.0  5.0  5.0   \n",
       "2157    260.3400    266.35  Ses05M_script03_2_M045     ang  1.0  5.0  5.0   \n",
       "\n",
       "                                                   text  \n",
       "0                                     Hi, I need an ID.  \n",
       "1     Okay, I'm sorry, but I just stood in this line...  \n",
       "2     No, they told me-I'm sorry, but they told me t...  \n",
       "3     What, I mean what... what's the difference?  W...  \n",
       "4                Can you just-can I just get the right-  \n",
       "...                                                 ...  \n",
       "2153  You are a vile tempered, wicked living, evil l...  \n",
       "2154                    Oh, you're not going like this.  \n",
       "2155                          [GARBAGE] No, you're not.  \n",
       "2156  oh! Marry you again? I wouldn't marry you agai...  \n",
       "2157  You're a wicked little vampire.  And I pray to...  \n",
       "\n",
       "[2158 rows x 8 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_data.to_csv('./dataset/preprocess/fusion_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_data = fusion_data.reset_index()\n",
    "fusion_data = fusion_data.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>val</th>\n",
       "      <th>act</th>\n",
       "      <th>dom</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6132</td>\n",
       "      <td>6.17</td>\n",
       "      <td>Ses05F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hi, I need an ID.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.1500</td>\n",
       "      <td>19.49</td>\n",
       "      <td>Ses05F_impro01_F001</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Okay, I'm sorry, but I just stood in this line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.8500</td>\n",
       "      <td>26.90</td>\n",
       "      <td>Ses05F_impro01_F002</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>No, they told me-I'm sorry, but they told me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.9800</td>\n",
       "      <td>34.46</td>\n",
       "      <td>Ses05F_impro01_F003</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>What, I mean what... what's the difference?  W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.4800</td>\n",
       "      <td>42.18</td>\n",
       "      <td>Ses05F_impro01_F004</td>\n",
       "      <td>fru</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Can you just-can I just get the right-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>236.5700</td>\n",
       "      <td>244.83</td>\n",
       "      <td>Ses05M_script03_2_M041</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You are a vile tempered, wicked living, evil l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>244.8400</td>\n",
       "      <td>246.58</td>\n",
       "      <td>Ses05M_script03_2_M042</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Oh, you're not going like this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>246.5900</td>\n",
       "      <td>248.83</td>\n",
       "      <td>Ses05M_script03_2_M043</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[GARBAGE] No, you're not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>255.8600</td>\n",
       "      <td>260.33</td>\n",
       "      <td>Ses05M_script03_2_M044</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>oh! Marry you again? I wouldn't marry you agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>260.3400</td>\n",
       "      <td>266.35</td>\n",
       "      <td>Ses05M_script03_2_M045</td>\n",
       "      <td>ang</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You're a wicked little vampire.  And I pray to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2158 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      start_time  end_time                wav_file emotion  val  act  dom  \\\n",
       "0         3.6132      6.17     Ses05F_impro01_F000     neu  4.0  2.5  3.0   \n",
       "1        14.1500     19.49     Ses05F_impro01_F001     fru  2.5  3.0  3.0   \n",
       "2        22.8500     26.90     Ses05F_impro01_F002     fru  2.5  3.0  3.5   \n",
       "3        29.9800     34.46     Ses05F_impro01_F003     fru  2.5  3.0  3.0   \n",
       "4        39.4800     42.18     Ses05F_impro01_F004     fru  2.5  3.5  3.0   \n",
       "...          ...       ...                     ...     ...  ...  ...  ...   \n",
       "2153    236.5700    244.83  Ses05M_script03_2_M041     ang  1.0  4.5  5.0   \n",
       "2154    244.8400    246.58  Ses05M_script03_2_M042     ang  1.0  4.5  4.5   \n",
       "2155    246.5900    248.83  Ses05M_script03_2_M043     ang  1.5  4.0  4.5   \n",
       "2156    255.8600    260.33  Ses05M_script03_2_M044     ang  1.0  5.0  5.0   \n",
       "2157    260.3400    266.35  Ses05M_script03_2_M045     ang  1.0  5.0  5.0   \n",
       "\n",
       "                                                   text  \n",
       "0                                     Hi, I need an ID.  \n",
       "1     Okay, I'm sorry, but I just stood in this line...  \n",
       "2     No, they told me-I'm sorry, but they told me t...  \n",
       "3     What, I mean what... what's the difference?  W...  \n",
       "4                Can you just-can I just get the right-  \n",
       "...                                                 ...  \n",
       "2153  You are a vile tempered, wicked living, evil l...  \n",
       "2154                    Oh, you're not going like this.  \n",
       "2155                          [GARBAGE] No, you're not.  \n",
       "2156  oh! Marry you again? I wouldn't marry you agai...  \n",
       "2157  You're a wicked little vampire.  And I pray to...  \n",
       "\n",
       "[2158 rows x 8 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stopword Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#english(Stopwords file)\n",
    "english_file = open('./dataset/english',mode='r')\n",
    "english_data = english_file.readlines()\n",
    "english_text = []\n",
    "\n",
    "for i in english_data:\n",
    "    english_text.append(i[:-1])\n",
    "\n",
    "english_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       start_time  end_time                wav_file emotion  val  act  dom  \\\n",
       "0         3.6132      6.17     Ses05F_impro01_F000     neu  4.0  2.5  3.0   \n",
       "1        14.1500     19.49     Ses05F_impro01_F001     fru  2.5  3.0  3.0   \n",
       "2        22.8500     26.90     Ses05F_impro01_F002     fru  2.5  3.0  3.5   \n",
       "3        29.9800     34.46     Ses05F_impro01_F003     fru  2.5  3.0  3.0   \n",
       "4        39.4800     42.18     Ses05F_impro01_F004     fru  2.5  3.5  3.0   \n",
       "...          ...       ...                     ...     ...  ...  ...  ...   \n",
       "2153    236.5700    244.83  Ses05M_script03_2_M041     ang  1.0  4.5  5.0   \n",
       "2154    244.8400    246.58  Ses05M_script03_2_M042     ang  1.0  4.5  4.5   \n",
       "2155    246.5900    248.83  Ses05M_script03_2_M043     ang  1.5  4.0  4.5   \n",
       "2156    255.8600    260.33  Ses05M_script03_2_M044     ang  1.0  5.0  5.0   \n",
       "2157    260.3400    266.35  Ses05M_script03_2_M045     ang  1.0  5.0  5.0   \n",
       "\n",
       "                                                   text  \n",
       "0                                                  need  \n",
       "1                        okay sorry stood line hour way  \n",
       "2                                  told sorry told line  \n",
       "3                                  mean difference need  \n",
       "4                                             get right  \n",
       "...                                                 ...  \n",
       "2153  vile tempered wicked living evil little beast ...  \n",
       "2154                                         going like  \n",
       "2155                                            garbage  \n",
       "2156              marry marry came crawling bended knee  \n",
       "2157  wicked little vampire pray god never set eye l...  \n",
       "\n",
       "[2158 rows x 8 columns]>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords Removement\n",
    "def clean_text(text):\n",
    "    #기호문자 제거\n",
    "    talk_text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # 소문자로 변경 후 분리\n",
    "    word_tokens = talk_text.lower().split()\n",
    "    #표제어 추출을 하기 위한 함수\n",
    "    le = WordNetLemmatizer()\n",
    "    #english_text 안의 영어 불용어를 집합으로 변화\n",
    "    stop_words = set(english_text)\n",
    "    word_tokens = [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    # 토큰화된 문장을 하나의 문장으로 변화\n",
    "    cleaned_text = \" \".join(word_tokens)\n",
    "    \n",
    "    # \\W:비문자 \\b:단어 경계(\\w와 \\W의 경계) \\w:문자\n",
    "    # compile 정규표현식을 컴파일 하는 함수\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    cleaned_text = shortword.sub('', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Stopwords Apply Content 열에 clean_review 함수를 통해 apply\n",
    "fusion_data['text'] = fusion_data['text'].apply(clean_text)\n",
    "fusion_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# raw_data의 'Content'열의 타입을 str 형태로 변경\n",
    "fusion_data['text'] = fusion_data['text'].astype('str')\n",
    "# raw_data의 'Content'열의 각 문장들을 단어로 tokenize\n",
    "fusion_data['text'] = fusion_data['text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['need']) list(['okay', 'sorry', 'stood', 'line', 'hour', 'way'])\n",
      " list(['told', 'sorry', 'told', 'line']) ... list(['garbage'])\n",
      " list(['marry', 'marry', 'came', 'crawling', 'bended', 'knee'])\n",
      " list(['wicked', 'little', 'vampire', 'pray', 'god', 'never', 'set', 'eye', 'long', 'live'])]\n"
     ]
    }
   ],
   "source": [
    "content_list = np.array(fusion_data['text'].tolist())\n",
    "print(content_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Tokenizing & word_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-de6856dcdc13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext_Xdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(content_list)\n",
    "\n",
    "text_Xdata = np.array(tokenizer.texts_to_sequences(content_list))\n",
    "\n",
    "print(text_Xdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
